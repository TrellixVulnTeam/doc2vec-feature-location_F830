
%     (compare to LDA and VSM)
%     i. effectiveness on gold sets (F-measure; Mean Average Precision & Precision at Rank 1?)
%	*the deep learning works better as the number of terms grows, so make sure those characteristics come out
%
%     ii. speed 
%           (for corpus building; for retrieval)


%
% Notes for Chris and Nick:
% 1) would be nice to also see the number of terms and number of unique terms per each project
% 2) i would definitely show both word2vec and doc2vec features and compare to LDA and VSM
% 3) make sure you mention the exact configuration of preprocessing steps (e.g. no word stemming)



\begin{table}
\input{tables/lda_mrr}
\caption{MRR of LDA for different number of topics}
\end{table}

\begin{table}
\input{tables/vec_mrr}
\caption{MRR of Doc2Vec inference for different numbers of vector features}
\end{table}

\begin{table}
\input{tables/vec_sums_mrr}
\caption{MRR of Doc2Vec vector summing for different numbers of vector features}
\end{table}


\input{tables/times}

% conclusing remarks:
%   doc2vec faster, could be easier to implement directly into an IDE search
%   tool
%   doc2vec inference needs more exploration for parameter tweaking
%   doc2vec vector summation, although slower, is at an acceptable time
