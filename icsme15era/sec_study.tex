
%     (compare to LDA and VSM)
%     i. effectiveness on gold sets (F-measure; Mean Average Precision & Precision at Rank 1?)
%	*the deep learning works better as the number of terms grows, so make sure those characteristics come out
%
%     ii. speed
%           (for corpus building; for retrieval)


%
% Notes for Chris and Nick:
% 1) would be nice to also see the number of terms and number of unique terms per each project
% 2) i would definitely show both word2vec and doc2vec features and compare to LDA and VSM
% 3) make sure you mention the exact configuration of preprocessing steps (e.g. no word stemming)


In this section we describe the design of a study in which we compare
a deep-learning-based FLT to a baseline topic-modeling-based FLT.  In
particular, we use a DV-based FLT and an LDA-based FLT, respectively.

%We describe the case study using the Goal-Question-Metric approach~\cite{Basili-etal_94}.

\subsection{Subject software systems}

We employ the dataset of six software systems by Dit et
al.~\cite{Dit-etal_2013}.  The dataset contains 633 queries for method-level
goldsets, as seen in Table~\ref{table:subjects}.  This dataset was automatically
extracted from changesets that relate to the queries (issue reports).

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\footnotesize
\centering
\caption{Subject System Sizes and Queries}
\begin{tabular}{lrrr}
    \toprule
    Subject System     & Methods & Queries    \\    %& Goldset Methods
    \midrule                                        %
    ArgoUML v0.22      & 12353    & 91        \\    %& 701
    ArgoUML v0.24      & 13064    & 52        \\    %& 357
    ArgoUML v0.26.2    & 16880    & 209       \\    %& 1560
    Jabref v2.6        & 5357     & 39        \\    %& 280
    jEdit v4.3         & 7305     & 150       \\    %& 748
    muCommander v0.8.5 & 8799     & 92        \\    %& 717
    \midrule                                        %
    Total              & 63758    & 633       \\    %& 4363
    \bottomrule
\end{tabular}
\label{table:subjects}
\end{table}

ArgoUML is a UML diagramming tool\footnote{\url{http://argouml.tigris.org/}}.
jEdit is a text editor\footnote{\url{http://www.jedit.org/}}.
JabRef is a BibTeX bibliography management tool\footnote{\url{http://jabref.sourceforge.net/}}.
muCommander is a cross-platform file manager\footnote{\url{http://www.mucommander.com/}}.

\subsection{Setting}

We implemented our approach in Python v2.7 using the topic modeling library,
Gensim~\cite{Rehurek-Sojk_2010} and our ANTLR v3 Java-based tool,
Teaser\footnote{\url{https://github.com/nkraft/teaser}}, for parsing source
code.

To build our corpora, we extract the documents representing methods from every
Java file in the snapshot.  The text of inner an method (e.g., a method inside
an anonymous class) is only attributed to that method, and not the containing
one.  Comments, literals, and identifiers within a method are considered as text
of the method.  Block comments immediately preceding an method are also included
in this text.

After extracting documents and tokenizing, we split the tokens based on camel
case, underscores, and non-letters.  We only keep the split tokens; original
tokens are discarded.  We normalize to lower case before filtering non-letters,
English stop words~\cite{Fox_1992}, Java keywords, and words shorter than three
characters long.  We do not stem words. Here, we must be careful to not lose
ordering of the words, as this is crucial for the deep learning approach.

For LDA, the approach is straightforward. We train the LDA model and query it
using it's built-in inference methods.
\todo{more blah blah here}

In Gensim, the DV deep learning model is known as Doc2Vec.
We train this \dv\ model on the corpora, and query it to obtain rankings of
methods related to the query. Because \dv\ is a neural network with multiple
layers (i.e., the document vector layer and the word vector layer), there are two
approaches for measuring document similarity.

For the document-based layer, we can infer the document vector of the query and
perform pair-wise similarity of it to the each method's document vector within
the model.  We also found it useful to consider the word-based layer. For this,
we can get the word vector for each word in the query and sum them. We then take
the query's summed vector and also perform pair-wise similarity to each document
vector in the model.

Regarding configuration of the two models, we choose various sizes of $K$ for
each, where $K$ for LDA is the number of topics and for \dv\ is the number of
features (i.e., document vector length).


\subsection{Data Collection and Analysis}

To evaluate the performance of an FLT we cannot use measures such as precision
and recall. This is because the FLT creates the rankings pairwise, causing every
entity being searched to appear in the rankings.  Poshyvanyk et al. define an
effectiveness measure that can be used for FLTs~\cite{Poshyvanyk-etal_2007}.
The effectiveness measure is the rank of the first relevant document and
represents the number of source code entities a developer would have to view
before reaching a relevant one.  The effectiveness measure allows evaluating the
FLT by using the mean reciprocal rank (MRR)~\cite{Voorhees_1999}: %, defined as:
\begin{equation}
    MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{e_i}
\end{equation}
where $Q$ is the set of queries and $e_i$ is the effectiveness measure for some
query $Q_i$.

We also want to explore the differences in terms of computational overhead for
using each model.  We collect information such as time taken to train the model
and the time taken to process the goldsets, i.e., the average time to query and
rank each subject system. We train and query all models on a system running
ArchLinux with an Intel Core-i7 4Hz processor and 32 GB of memory.

\subsection{Results and Discussion}

\input{tables/times}

\input{tables/mrr_mega}

Table~\ref{tab:mrr} summarizes the results of each subject system for each model
and query approach. We bold which of the three approaches is greatest for each
value $K$ at steps of $100$. To the right of Table~\ref{tab:mrr} is
Figure~\ref{fig:mrr}, a graphical view of the all values of $K$ at steps of
$25$.

There are a handful of clear trends. For example, the ArgoUML projects perform
similarily across each version. For all three versions, the \dv\ word vector
summation performs best for many values of $K$, with few exceptions in favor of
LDA.  Interestingly, \dv\ inference has the worst performance across all three
systems. This trend continues with JabRef and jEdit, but does not for
muCommander.  Surprisingly, \dv\ inference always performs best for muCommander.

A second trend is how little features, $K$, \dv\ needs in order to perform well.
As shown in Figure~\ref{fig:mrr}, many of the projects achieve high \dv\ 
summation performance by $K=100$, and plateaus after that. This is in contrast
to LDA, which can require as many as 300 to 500 topics.

Tables~\ref{tab:trainingtimes} and~\ref{tab:querytimes} summarizes the time taken
to train each model on the corpus and the average query and rank time for all
queries in the goldset.

To train a model is almost inconsequential with \dv, as it finishes in under
3 seconds for all systems. LDA can take up to nearly a minute and a half.
The average time to query and rank all queries in the goldset shows that \dv\ 
inference is fastest, while the word vector summation is on par or worse than
LDA.

